{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeepkhamari/llm_deploment_using_jupiter_notebook/blob/main/llm_deployment_huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ec9de2c",
      "metadata": {
        "id": "4ec9de2c"
      },
      "source": [
        "# LLM Deployment with HuggingFace\n",
        "\n",
        "This notebook demonstrates end-to-end LLM deployment using HuggingFace Transformers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EncGTvSocgrV",
        "outputId": "bd3122fe-08f6-403d-e178-3307a7e6041c"
      },
      "id": "EncGTvSocgrV",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers fastapi uvicorn pydantic"
      ],
      "metadata": {
        "id": "y6FZ9jEfcnwx"
      },
      "id": "y6FZ9jEfcnwx",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "MODEL_NAME = \"distilgpt2\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "def generate_text(prompt, max_new_tokens=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            top_p=0.95\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "app = FastAPI(title=\"LLM Inference API\")\n",
        "\n",
        "class PromptRequest(BaseModel):\n",
        "    prompt: str\n",
        "    max_tokens: int = 50\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "def generate(req: PromptRequest):\n",
        "    return {\"response\": generate_text(req.prompt, req.max_tokens)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMYLy-VQcrur",
        "outputId": "f8c3bd69-c77c-4902-d1a9-459fe2292c10"
      },
      "id": "bMYLy-VQcrur",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-Izg5H8cwso",
        "outputId": "cb39f1a3-adb7-4aff-e5c5-5ddd944f044c"
      },
      "id": "E-Izg5H8cwso",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\t__pycache__  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uvicorn app:app --host 0.0.0.0 --port 8000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_4uHgaVcz93",
        "outputId": "a54c7f9c-57cf-4d1f-bfb4-a9ddc0beed07"
      },
      "id": "e_4uHgaVcz93",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rtokenizer_config.json:   0% 0.00/26.0 [00:00<?, ?B/s]\rtokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 113kB/s]\n",
            "config.json: 100% 762/762 [00:00<00:00, 5.97MB/s]\n",
            "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 20.6MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 59.1MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 30.6MB/s]\n",
            "2025-12-14 06:04:48.092652: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765692288.110352     632 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765692288.115341     632 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765692288.130265     632 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765692288.130292     632 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765692288.130296     632 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765692288.130301     632 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-14 06:04:48.134821: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 353M/353M [00:02<00:00, 174MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 949kB/s]\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m632\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
            "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m632\u001b[0m]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!mv cloudflared-linux-amd64 /usr/local/bin/cloudflared"
      ],
      "metadata": {
        "id": "AO3C0ydzc4J2"
      },
      "id": "AO3C0ydzc4J2",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cloudflared --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1SI_gp7c7Fd",
        "outputId": "5c3adea1-9f00-4cb3-c148-475156cfccd7"
      },
      "id": "J1SI_gp7c7Fd",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cloudflared version 2025.11.1 (built 2025-11-07-16:59 UTC)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cloudflared tunnel --url http://localhost:8000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66dOGoIXc-Uv",
        "outputId": "c3dd21d8-f0ac-4a93-b8ee-d276d83935f7"
      },
      "id": "66dOGoIXc-Uv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2025-12-14T06:16:21Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-12-14T06:16:21Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-12-14T06:16:24Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-12-14T06:16:24Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-12-14T06:16:24Z\u001b[0m \u001b[32mINF\u001b[0m |  https://translation-lewis-context-plastics.trycloudflare.com                              |\n",
            "\u001b[90m2025-12-14T06:16:24Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-12-14T06:16:24Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-12-14T06:16:24Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.11.1 (Checksum 991dffd8889ee9f0147b6b48933da9e4407e68ea8c6d984f55fa2d3db4bb431d)\n",
            "\u001b[90m2025-12-14T06:16:24Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.9, GoArch: amd64\n",
            "\u001b[90m2025-12-14T06:16:24Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:8000]\n",
            "\u001b[90m2025-12-14T06:16:24Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-12-14T06:16:24Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: d5ca7cff-6b59-43a7-b6fa-2069ac3198d3\n",
            "\u001b[90m2025-12-14T06:16:25Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-12-14T06:16:25Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-12-14T06:16:25Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-12-14T06:16:25Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2025-12-14T06:16:25Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-12-14T06:16:25Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-12-14T06:16:25Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-12-14T06:16:25Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77\n",
            "2025/12/14 06:16:25 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-12-14T06:16:25Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m08a0c4bf-e1a0-4c63-b34d-cd5650f81b7f \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77 \u001b[36mlocation=\u001b[0matl06 \u001b[36mprotocol=\u001b[0mquic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "https://xxxxx.trycloudflare.com/docs"
      ],
      "metadata": {
        "id": "rEqCddNsdBZM"
      },
      "id": "rEqCddNsdBZM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://translation-lewis-context-plastics.trycloudflare.com/generate\"\n",
        "payload = {\n",
        "    \"prompt\": \"Explain MPI parallel programming in simple terms\",\n",
        "    \"max_tokens\": 50\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=payload)\n",
        "response.json()"
      ],
      "metadata": {
        "id": "qkaKY5YbdIBS"
      },
      "id": "qkaKY5YbdIBS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(model.parameters()).device"
      ],
      "metadata": {
        "id": "twlIb-VXdJLL"
      },
      "id": "twlIb-VXdJLL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}